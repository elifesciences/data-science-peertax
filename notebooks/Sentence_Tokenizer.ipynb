{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of sentences (preparation for LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.! This notebook requires a dataframe of \"sentencized\" texts.\n",
    "\n",
    "Run \"Text_Sentencizer.ipynb\" and create a \"sentencized.tsv\" before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24542 entries, 0 to 24541\n",
      "Data columns (total 3 columns):\n",
      "manuscript_ID    24542 non-null object\n",
      "review_ID        24542 non-null object\n",
      "sentences        24542 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 575.3+ KB\n"
     ]
    }
   ],
   "source": [
    "flatten_df = pd.read_csv('../pickles/wellcome_sentencized.tsv',sep='\\t',quoting=csv.QUOTE_NONE)\n",
    "flatten_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "flatten_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove standard sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23369 entries, 0 to 23368\n",
      "Data columns (total 3 columns):\n",
      "manuscript_ID    23369 non-null object\n",
      "review_ID        23369 non-null object\n",
      "sentences        23369 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 547.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "std_sentence = ['I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.',\n",
    "                'I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.',\n",
    "                'I confirm that I have read this submission and believe that I have an appropriate level of expertise to state that I do not consider it to be of an acceptable scientific standard, for reasons outlined above.',\n",
    "                'We confirm that we have read this submission and believe that we have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however we have significant reservations, as outlined above.',                \n",
    "                'We confirm that we have read this submission and believe that we have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.',\n",
    "                'We confirm that we have read this submission and believe that we have an appropriate level of expertise to state that we do not consider it to be of an acceptable scientific standard, for reasons outlined above.',\n",
    "                'I believe that I have an appropriate levels of expertise to determine whether or not it meets an acceptable scientific standard.',\n",
    "                'I believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.',\n",
    "                'Are sufficient details of methods and analysis provided to allow replication by others?',\n",
    "                'Are sufficient details of the methods and analysis provided to allow replication by others?',\n",
    "                'Are sufficient details of methods and materials provided to allow replication by others?',\n",
    "                'Are sufficient details of the methods provided to allow replication by others?',\n",
    "                'There are generally sufficient details of the methods and analysis provide to allow replication by others.',\n",
    "                'Are the conclusions drawn adequately supported by the results?'\n",
    "               ]\n",
    "\n",
    "def remove_std_sent(text, std_sentence):\n",
    "    for sent in std_sentence:\n",
    "        patt = re.compile(sent)\n",
    "        text = re.sub(patt, '', str(text))\n",
    "\n",
    "flatten_df = flatten_df[~flatten_df.sentences.isin(std_sentence)]\n",
    "flatten_df['sentences'].apply(lambda x: remove_std_sent(x,std_sentence))\n",
    "flatten_df.reset_index(drop=True,inplace=True)\n",
    "flatten_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2951589b6ff74e3dbd84249d015494e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from peertax.tokenizer_LDA import custom_tokenizer as ct\n",
    "from random import randint\n",
    "num = randint(0,len(flatten_df))\n",
    "sent_test = [flatten_df.loc[num,'sentences']]\n",
    "sent_after = ct(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thus, reproducibility was not established, and the experiment is underpowered.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reproducibility establish experiment underpowered']\n"
     ]
    }
   ],
   "source": [
    "print(sent_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ef38305c85499193d967f9d88d4159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23369), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to clean up everything: 1.99 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "txt = ct(flatten_df['sentences'])\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the results in a DataFrame to remove missing values.\n",
    "\n",
    "Don't remove duplicates because they are still reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22858 entries, 0 to 23368\n",
      "Data columns (total 1 columns):\n",
      "token    22858 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 357.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'token': txt})\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with initial dataset to retain indexing. Assign a provisional 'token' column (will update after creating bigram and trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22858 entries, 0 to 22857\n",
      "Data columns (total 4 columns):\n",
      "manuscript_ID    22858 non-null object\n",
      "review_ID        22858 non-null object\n",
      "sentences        22858 non-null object\n",
      "token            22858 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 714.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = pd.concat([flatten_df, df_clean], axis=1, join='inner').reset_index(drop=True)\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manuscript_ID</th>\n",
       "      <th>review_ID</th>\n",
       "      <th>sentences</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The Deciphering Mechanisms of Developmental Di...</td>\n",
       "      <td>deciphering mechanisms developmental disorders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The lines chosen for this study were selected ...</td>\n",
       "      <td>line choose study select homozygous lethal sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The authors employ High Resolution Episcopic M...</td>\n",
       "      <td>author employ high resolution episcopic micros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>They exploit this rich dataset with a systemat...</td>\n",
       "      <td>exploit rich dataset systematic depth annotati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The result is a survey of impressive scope in ...</td>\n",
       "      <td>result survey impressive scope term annotation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     manuscript_ID                              review_ID  \\\n",
       "0  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "1  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "2  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "3  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "4  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  The Deciphering Mechanisms of Developmental Di...   \n",
       "1  The lines chosen for this study were selected ...   \n",
       "2  The authors employ High Resolution Episcopic M...   \n",
       "3  They exploit this rich dataset with a systemat...   \n",
       "4  The result is a survey of impressive scope in ...   \n",
       "\n",
       "                                               token  \n",
       "0  deciphering mechanisms developmental disorders...  \n",
       "1  line choose study select homozygous lethal sub...  \n",
       "2  author employ high resolution episcopic micros...  \n",
       "3  exploit rich dataset systematic depth annotati...  \n",
       "4  result survey impressive scope term annotation...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08:14:40: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_cleaned['token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08:14:40: collecting all words and their counts\n",
      "INFO - 08:14:40: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 08:14:41: collected 180477 word types from a corpus of 254818 words (unigram + bigrams) and 22858 sentences\n",
      "INFO - 08:14:41: using 180477 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases_bi = Phrases(sent, min_count=30, progress_per=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08:14:41: source_vocab length 180477\n",
      "INFO - 08:14:45: Phraser built with 55 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08:14:45: collecting all words and their counts\n",
      "INFO - 08:14:45: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 08:14:48: collected 181304 word types from a corpus of 251730 words (unigram + bigrams) and 22858 sentences\n",
      "INFO - 08:14:48: using 181304 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases_tri = Phrases(phrases_bi[sent], min_count=30, progress_per=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08:14:48: source_vocab length 181304\n",
      "INFO - 08:14:52: Phraser built with 55 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "trigram = Phraser(phrases_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the corpus based on the bigrams & trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = trigram[bigram[sent]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run figure_conv() to convert bigrams (like fig_a etc.) found during last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_conv(text):\n",
    "    if text in ['figure','figure_a','figure_b','figure_c',\n",
    "                'fig','fig_a','fig_b','fig_c','figure_figure_supplement']:\n",
    "        return 'figure'\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def figure_conv_array(doc):\n",
    "    return [figure_conv(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [figure_conv_array(r) for r in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do sanity check of the effectiveness of the cleaning and addition of bigrams & trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14657"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author',\n",
       " 'study',\n",
       " 'datum',\n",
       " 'result',\n",
       " 'paper',\n",
       " 'figure',\n",
       " 'analysis',\n",
       " 'include',\n",
       " 'provide',\n",
       " 'method',\n",
       " 'need',\n",
       " 'report',\n",
       " 'cell',\n",
       " 'present',\n",
       " 'use',\n",
       " 'different',\n",
       " 'describe',\n",
       " 'important',\n",
       " 'research',\n",
       " 'number']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'token' column with new, actual 'tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22858 entries, 0 to 22857\n",
      "Data columns (total 4 columns):\n",
      "manuscript_ID    22858 non-null object\n",
      "review_ID        22858 non-null object\n",
      "sentences        22858 non-null object\n",
      "token            22858 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 714.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned['token'] = [r for r in sentences]\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condense 'token' column as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manuscript_ID</th>\n",
       "      <th>review_ID</th>\n",
       "      <th>sentences</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The Deciphering Mechanisms of Developmental Di...</td>\n",
       "      <td>deciphering,mechanisms,developmental,disorders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The lines chosen for this study were selected ...</td>\n",
       "      <td>line,choose,study,select,homozygous,lethal,sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The authors employ High Resolution Episcopic M...</td>\n",
       "      <td>author,employ,high,resolution,episcopic,micros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>They exploit this rich dataset with a systemat...</td>\n",
       "      <td>exploit,rich,dataset,systematic,depth,annotati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.12688/wellcomeopenres.9899.1</td>\n",
       "      <td>10.21956/wellcomeopenres.10670.r18405</td>\n",
       "      <td>The result is a survey of impressive scope in ...</td>\n",
       "      <td>result,survey,impressive,scope,term,annotation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     manuscript_ID                              review_ID  \\\n",
       "0  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "1  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "2  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "3  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "4  10.12688/wellcomeopenres.9899.1  10.21956/wellcomeopenres.10670.r18405   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  The Deciphering Mechanisms of Developmental Di...   \n",
       "1  The lines chosen for this study were selected ...   \n",
       "2  The authors employ High Resolution Episcopic M...   \n",
       "3  They exploit this rich dataset with a systemat...   \n",
       "4  The result is a survey of impressive scope in ...   \n",
       "\n",
       "                                               token  \n",
       "0  deciphering,mechanisms,developmental,disorders...  \n",
       "1  line,choose,study,select,homozygous,lethal,sub...  \n",
       "2  author,employ,high,resolution,episcopic,micros...  \n",
       "3  exploit,rich,dataset,systematic,depth,annotati...  \n",
       "4  result,survey,impressive,scope,term,annotation...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['token'] = df_cleaned['token'].str.join(',')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_tsv = \"../pickles/wellcome_tokenized_LDA_sentence_0.tsv\"\n",
    "df_cleaned.to_csv(path_save_tsv, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
